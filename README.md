# Wizardry School Admissions Prediction

A supervised machine learning project that predicts which aspiring students are admitted to a prestigious **wizardry school**.  
The project applies a complete ML pipeline: data preprocessing, feature engineering, feature selection, model training & tuning, and evaluation on an **imbalanced dataset**.

Final model: **Decision Tree Classifier**, selected for its interpretability and balanced performance across accuracy, F1, precision, and recall.

---

## ðŸ§° Tech Stack

- **Python 3.10+**
- **Jupyter Notebook**
- **scikit-learn** â€“ model building, preprocessing, feature selection, cross-validation
- **XGBoost** â€“ gradient boosting model
- **imbalanced-learn** â€“ handling dataset imbalance
- **NumPy / Pandas** â€“ data manipulation
- **Matplotlib / Seaborn** â€“ visualization

---

## ðŸ“ Repository Structure

~~~text
.
â”œâ”€ data/
â”‚  â””â”€ train.csv
â”‚  â””â”€ test.csv
â”œâ”€ notebooks/
â”‚  â””â”€ ML1_Group08_Notebook.ipynb   # main notebook with pipeline
â”œâ”€ report/
â”‚  â””â”€ ML1_Group08_Report.pdf       # detailed report
â”œâ”€ requirements.txt
â”œâ”€ model_metrics_xslx
â”œâ”€ README.md
â””â”€ LICENSE
~~~

---

## ðŸ”Ž Methodology

1. **Data Preparation**  
   - Missing value imputation (KNNImputer).  
   - Removal of low-value features (>50% missing).  
   - Data type optimization.  

2. **Exploration & Feature Engineering**  
   - Outlier detection & visualization.  
   - New features: *Experience Level Category*, *Financial Background Category*.  

3. **Feature Selection**  
   - Filter methods (variance, correlation, chi-square).  
   - Wrapper methods (RFE, Sequential Feature Selection).  
   - Embedded methods (Lasso, Decision Tree feature importances).  
   - Majority voting for final predictor set.  

4. **Modeling**  
   - Algorithms tested: Decision Tree, Extra Trees, Random Forest, Gradient Boosting, XGBoost, SVM, MLP.  
   - Hyperparameter tuning with GridSearchCV.  
   - Evaluation with K-Fold and Repeated K-Fold CV.  

5. **Evaluation Metrics**  
   - F1 Score (main metric for imbalanced data).  
   - Precision, Recall, Balanced Accuracy.  
   - Confusion Matrix.  

---

## ðŸ“Š Results

- **Best Model:** Decision Tree Classifier  
  - Tuned hyperparameters: `criterion="gini"`, `max_depth=5`, `max_leaf_nodes=7`, `min_samples_split=2`.  
  - Validation F1 Score: ~0.70  
  - Test F1 Score (Kaggle subset): 0.8888  
- Ensemble models like XGBoost and Gradient Boosting showed strong results, but the Decision Tree was chosen for interpretability.  
- Handling imbalance remains a future direction (oversampling, undersampling).  

---

## ðŸš€ How to Run

1. Clone this repository:  
   ~~~bash
   git clone https://github.com/<your-username>/wizardry-school-admissions-ml.git
   cd wizardry-school-admissions-ml
   ~~~

2. Install dependencies:  
   ~~~bash
   pip install -r requirements.txt
   ~~~

3. Open the notebook:  
   ~~~bash
   jupyter notebook notebooks/ML1_Group08_Notebook.ipynb
   ~~~

---

## âœ… Requirements

See [`requirements.txt`](./requirements.txt).

---

## ðŸ“„ License

This project is under MIT License

